<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>class affinity maximum likelihood text scaling model — textmodel_affinity • quanteda</title>

<!-- jquery -->
<script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">


<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script>
<script src="../pkgdown.js"></script>
  
  
<!-- mathjax -->
<script src='https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


<!-- Google analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-144616-24', 'auto');
  ga('send', 'pageview');

</script>

  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">quanteda</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/pkgdown_only/comparison-packages.html">Comparing quanteda to other packages</a>
    </li>
    <li>
      <a href="../articles/pkgdown_only/design.html">quanteda Structure and Design</a>
    </li>
    <li>
      <a href="../articles/pkgdown_only/LitVignette.html">Replicating *Text Analysis with R for Students of Literature*</a>
    </li>
    <li>
      <a href="../articles/pkgdown_only/plotting.html">Plotting with quanteda</a>
    </li>
    <li>
      <a href="../articles/quickstart.html">Getting Started with quanteda</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

      <div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>class affinity maximum likelihood text scaling model</h1>
    </div>

    
    <p><code>textmodel_affinity</code> implements the maximum likelihood supervised text
scaling method described in Perry and Benoit (2017).</p>
    

    <pre class="usage"><span class='fu'>textmodel_affinity</span>(<span class='no'>x</span>, <span class='no'>y</span>, <span class='kw'>exclude</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>smooth</span> <span class='kw'>=</span> <span class='fl'>0.5</span>, <span class='kw'>ref_smooth</span> <span class='kw'>=</span> <span class='fl'>0.5</span>,
  <span class='kw'>verbose</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>)</pre>
    
    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a> Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>x</th>
      <td><p>the <a href='dfm.html'>dfm</a> or <a href='bootstrap_dfm.html'>bootstrap_dfm</a> object on which the model
will be fit.  Does not need to contain only the training documents, since
the index of these will be matched automatically.</p></td>
    </tr>
    <tr>
      <th>y</th>
      <td><p>vector of training classes/scores associated with each document
identified in <code>data</code></p></td>
    </tr>
    <tr>
      <th>exclude</th>
      <td><p>a set of words to exclude from the model</p></td>
    </tr>
    <tr>
      <th>smooth</th>
      <td><p>a smoothing parameter for class affinities; defaults to 0.5
(Jeffreys prior). A plausible alternative would be 1.0 (Laplace prior).</p></td>
    </tr>
    <tr>
      <th>ref_smooth</th>
      <td><p>a smoothing parameter for token distributions;
defaults to 0.5</p></td>
    </tr>
    <tr>
      <th>verbose</th>
      <td><p>logical; if <code>TRUE</code> print diagnostic information during
fitting.</p></td>
    </tr>
    </table>
    
    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    <p>Perry, Patrick O. and Kenneth Benoit.  (2017) "Scaling Text with
  the Class Affinity Model".
  <a href='http://arxiv.org/abs/1710.08963'>arXiv:1710.08963 [stat.ML]</a>.</p>
    

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'>(<span class='no'>fitted</span> <span class='kw'>&lt;-</span> <span class='fu'>textmodel_affinity</span>(<span class='no'>data_dfm_lbgexample</span>, <span class='kw'>y</span> <span class='kw'>=</span> <span class='fu'>c</span>(<span class='st'>"L"</span>, <span class='fl'>NA</span>, <span class='fl'>NA</span>, <span class='fl'>NA</span>, <span class='st'>"R"</span>, <span class='fl'>NA</span>)))</div><div class='output co'>#&gt; Call:	
#&gt; textmodel_affinity.dfm(x = data_dfm_lbgexample, y = c("L", NA, 
#&gt;     NA, NA, "R", NA))
#&gt; 
#&gt;     Training documents per class: L: 1, R: 1
#&gt;          Total training features: 37</div><div class='input'><span class='fu'>predict</span>(<span class='no'>fitted</span>)</div><div class='output co'>#&gt; Predicted textmodel of type: affinity
#&gt; 
#&gt; Estimated coefficients:
#&gt; 
#&gt;          L    s.e.       R    s.e.    chi2
#&gt; R1 0.99950 0.00071 0.00050 0.00071     9.3
#&gt; R2 0.99941 0.00083 0.00059 0.00083  9415.8
#&gt; R3 0.50000 0.02734 0.50000 0.02734 24864.5
#&gt; R4 0.00059 0.00083 0.99941 0.00083  9415.8
#&gt; R5 0.00050 0.00071 0.99950 0.00071     9.3
#&gt; V1 0.99867 0.00187 0.00133 0.00187 19458.5
#&gt; 
#&gt; Some diagnostics here about how many words were not found in training vocabulary.</div><div class='input'><span class='fu'>predict</span>(<span class='no'>fitted</span>, <span class='kw'>newdata</span> <span class='kw'>=</span> <span class='no'>data_dfm_lbgexample</span>[<span class='fl'>6</span>, ])</div><div class='output co'>#&gt; Predicted textmodel of type: affinity
#&gt; 
#&gt; Estimated coefficients:
#&gt; 
#&gt;    L   s.e.      R   s.e.  chi2
#&gt; V1 1 0.0019 0.0013 0.0019 19459
#&gt; 
#&gt; Some diagnostics here about how many words were not found in training vocabulary.</div><div class='input'>
<span class='co'># compute bootstrapped SEs</span>
<span class='no'>bsdfm</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='bootstrap_dfm.html'>bootstrap_dfm</a></span>(<span class='no'>data_corpus_dailnoconf1991</span>, <span class='kw'>n</span> <span class='kw'>=</span> <span class='fl'>10</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)
<span class='fu'>textmodel_affinity</span>(<span class='no'>bsdfm</span>, <span class='kw'>y</span> <span class='kw'>=</span> <span class='fu'>c</span>(<span class='st'>"Govt"</span>, <span class='st'>"Opp"</span>, <span class='st'>"Opp"</span>, <span class='fu'>rep</span>(<span class='fl'>NA</span>, <span class='fl'>55</span>)))</div><div class='output co'>#&gt; <span class='message'>Bootstrapping textmodel_affinity for 58 documents:</span></div><div class='output co'>#&gt; <span class='message'>   ...computing model for dfm from original texts</span></div><div class='output co'>#&gt; <span class='message'>   ...computing bootstrapped models and saving coefficients</span>
#&gt; <span class='message'>     </span></div><div class='output co'>#&gt; <span class='message'> 1</span></div><div class='output co'>#&gt; <span class='message'> 2</span></div><div class='output co'>#&gt; <span class='message'> 3</span></div><div class='output co'>#&gt; <span class='message'> 4</span></div><div class='output co'>#&gt; <span class='message'> 5</span></div><div class='output co'>#&gt; <span class='message'> 6</span></div><div class='output co'>#&gt; <span class='message'> 7</span></div><div class='output co'>#&gt; <span class='message'> 8</span></div><div class='output co'>#&gt; <span class='message'> 9</span></div><div class='output co'>#&gt; <span class='message'> 10</span></div><div class='output co'>#&gt; <span class='message'> 11</span></div><div class='output co'>#&gt; <span class='message'>   ...replacing original SEs with bootstrapped SEs</span></div><div class='output co'>#&gt; <span class='message'>   ...finished.</span></div><div class='output co'>#&gt; Predicted textmodel of type: affinity
#&gt; (showing 30 of 58 documents)
#&gt; 
#&gt; Estimated coefficients:
#&gt; 
#&gt;                              Govt    s.e.     Opp    s.e.  chi2
#&gt; Haughey_FF_Taois.txt      0.99955 3.2e-05 0.00045 3.2e-05   874
#&gt; Spring_Lab_Leader.txt     0.00044 2.8e-05 0.99956 2.8e-05  1847
#&gt; deRossa_DL_Leader.txt     0.00047 4.8e-05 0.99953 4.8e-05  1879
#&gt; vAhearn_FG_.txt           0.18175 3.9e-02 0.81825 3.9e-02  3346
#&gt; vAhernD_FF_.txt           0.20235 4.8e-02 0.79765 4.8e-02  4279
#&gt; vBarry_FG_.txt            0.44786 8.9e-02 0.55214 8.9e-02  3513
#&gt; vBlaney_FF_Indep.txt      0.28803 4.9e-02 0.71197 4.9e-02  4374
#&gt; vBoylan_FG_.txt           0.18318 5.6e-02 0.81682 5.6e-02  3338
#&gt; vBrennan_FF_Min.txt       0.39177 4.9e-02 0.60823 4.9e-02  4862
#&gt; vBrowne_FG_.txt           0.26126 8.4e-02 0.73874 8.4e-02  3605
#&gt; vBrutonJ_FG_Leader.txt    0.27156 3.2e-02 0.72844 3.2e-02  4722
#&gt; vBrutonR_FG_.txt          0.22384 6.2e-02 0.77616 6.2e-02  2976
#&gt; vBurke_FF_Min.txt         0.37467 3.0e-02 0.62533 3.0e-02  7914
#&gt; vCollins_FF_Min.txt       0.71582 1.7e-01 0.28418 1.7e-01  5264
#&gt; vConnaughton_FG_.txt      0.17719 6.4e-02 0.82281 6.4e-02  3080
#&gt; vCowen_FF_.txt            0.53006 1.4e-01 0.46994 1.4e-01  3327
#&gt; vCreed_FG_.txt            0.30043 2.2e-02 0.69957 2.2e-02  3306
#&gt; vCullimore_FF_.txt        0.58683 1.4e-01 0.41317 1.4e-01  2253
#&gt; vCurrie_FG_.txt           0.13869 5.6e-02 0.86131 5.6e-02  2705
#&gt; vDaly_FF_Min.txt          0.53370 1.2e-01 0.46630 1.2e-01  4979
#&gt; vDavern_FF_.txt           0.15070 6.2e-02 0.84930 6.2e-02  3300
#&gt; vDeasy_FG_.txt            0.13819 6.2e-02 0.86181 6.2e-02  3849
#&gt; vDeenihan_FG_.txt         0.18397 7.5e-02 0.81603 7.5e-02  2397
#&gt; vDurkan_FG_.txt           0.06387 1.1e-01 0.93613 1.1e-01  2745
#&gt; vFerris_Lab_.txt          0.16892 6.7e-02 0.83108 6.7e-02  4033
#&gt; vFinucane_FG_.txt         0.15331 6.5e-02 0.84669 6.5e-02  3193
#&gt; vFitzGerald_FG_.txt       0.44962 8.6e-02 0.55038 8.6e-02  3662
#&gt; vFlaherty_FG_.txt         0.20939 5.6e-02 0.79061 5.6e-02  3525
#&gt; vFlynn_FF_Min.txt         0.43254 6.8e-02 0.56746 6.8e-02  5463
#&gt; vGarland_Green_Leader.txt 0.27677 6.1e-02 0.72323 6.1e-02  3599
#&gt; vGilmore_DL_.txt          0.25704 4.1e-02 0.74296 4.1e-02  4036
#&gt; vHigginsJ_FG_.txt         0.15266 5.5e-02 0.84734 5.5e-02 16156
#&gt; vHigginsMD_Lab_.txt       0.17709 6.0e-02 0.82291 6.0e-02  3672
#&gt; vHillery_FF_.txt          0.53479 9.0e-02 0.46521 9.0e-02  6744
#&gt; vHowlin_Lab_.txt          0.23440 5.2e-02 0.76560 5.2e-02  2776
#&gt; vKenny_FG_.txt            0.29380 7.6e-02 0.70620 7.6e-02  2473
#&gt; vLenihan_FF_.txt          0.34527 5.4e-02 0.65473 5.4e-02  5127
#&gt; vLeyden_FF_JMin.txt       0.52413 1.0e-01 0.47587 1.0e-01  3828
#&gt; vMcDaid_FF_.txt           0.17517 7.7e-02 0.82483 7.7e-02  4042
#&gt; vNolan_FF_.txt            0.79026 2.1e-01 0.20974 2.1e-01  2379
#&gt; vNoonan_FG_.txt           0.22302 6.3e-02 0.77698 6.3e-02  3801
#&gt; vODonoghue_FF_.txt        0.27694 4.8e-02 0.72306 4.8e-02  2755
#&gt; vOHanlon_FF_Min.txt       0.62640 1.4e-01 0.37360 1.4e-01  8316
#&gt; vOKennedy_FF_Min.txt      0.50431 8.4e-02 0.49569 8.4e-02  5860
#&gt; vOMalley_PD_Leader.txt    0.44588 6.4e-02 0.55412 6.4e-02  4100
#&gt; vORourke_FF_Min.txt       0.41646 5.8e-02 0.58354 5.8e-02  8401
#&gt; vOShea_Lab_.txt           0.12647 7.6e-02 0.87353 7.6e-02  6645
#&gt; vOwen_FG_.txt             0.11156 6.8e-02 0.88844 6.8e-02  6885
#&gt; vQuinn_Lab_.txt           0.29982 2.7e-02 0.70018 2.7e-02  3783
#&gt; vRabbitte_DL_.txt         0.07838 1.3e-01 0.92162 1.3e-01  4262
#&gt; vReynoldsG_FG_.txt        0.17913 2.4e-02 0.82087 2.4e-02  2339
#&gt; vReynolds_FF_Min.txt      0.68655 1.5e-01 0.31345 1.5e-01  4036
#&gt; vRoche_FF_.txt            0.20856 4.8e-02 0.79144 4.8e-02  4976
#&gt; vStagg_Lab_.txt           0.43570 8.4e-02 0.56430 8.4e-02  5393
#&gt; vTaylorQuinn_FG_.txt      0.20615 4.5e-02 0.79385 4.5e-02  3522
#&gt; vWilson_FF_Tan.txt        0.72483 1.8e-01 0.27517 1.8e-01  3533
#&gt; vWoods_FF_Min.txt         0.59174 1.4e-01 0.40826 1.4e-01  4226
#&gt; vYates_FG_.txt            0.38047 3.3e-02 0.61953 3.3e-02  3860
#&gt; 
#&gt; Some diagnostics here about how many words were not found in training vocabulary.</div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      
      <li><a href="#references">References</a></li>
      
      <li><a href="#examples">Examples</a></li>
    </ul>

    <h2>Author</h2>
    
Patrick Perry

  </div>
</div>

      <footer>
      <div class="copyright">
  <p>Developed by Kenneth Benoit.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
   </div>

  </body>
</html>
