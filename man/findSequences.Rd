% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/findSequences.R
\name{findSequences}
\alias{findSequences}
\title{find sequences of tokens}
\usage{
findSequences(x, tokens, count_min, smooth = 0.001, nested = TRUE)
}
\arguments{
\item{x}{tokenizedTexts objects}

\item{tokens}{types of token in sequuences}

\item{count_min}{minimum frequency of sequences}

\item{smooth}{smoothing factor}

\item{nested}{collect nested sub-sequence}
}
\description{
This function automatically identify sequences of tokens. This algorithm is 
based on Blaheta and Johnson's <U+9225><U+6DEF>nsupervised Learning of Multi-Word Verbs<U+9225><U+FFFD>.
}
\examples{
sents <- as.character(tokens(data_corpus_inaugural[1:10], what = "sentence"))
tokens <- tokens(sents, removePunct = TRUE)
tokens <- tokens_select(tokens, stopwords("english"), "remove", padding = TRUE)
types <- unique(as.character(tokens))

# extracting multi-part nouns
types_upper <- types[stringi::stri_detect_regex(types, "^([A-Z][a-z\\\\-]{2,})")]
seqs <- findSequences(as.tokenizedTexts(tokens), types_upper, count_min = 2)
head(seqs, 10)

# types can be any words
types_lower <- types[stringi::stri_detect_regex(types, "^([a-z]+)$") & !types \%in\%stopwords()]
seqs2 <- findSequences(as.tokenizedTexts(tokens), types_lower, count_min = 3)
head(seqs2, 10)

}
\author{
Kohei Watanabe
}
\references{
Blaheta, D., & Johnson, M. (2001). Unsupervised learning of
  multi-word verbs. Presented at the ACLEACL Workshop on the Computational
  Extraction, Analysis and Exploitation of Collocations.
}
\keyword{collocations}
\keyword{internal}

