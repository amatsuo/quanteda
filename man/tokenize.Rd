% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/tokenize.R
\name{tokenizeOld.corpus}
\alias{tokenise}
\alias{tokenize}
\alias{tokenize.character}
\alias{tokenizeOld.corpus}
\title{tokenize a set of texts}
\usage{
\method{tokenizeOld}{corpus}(x, ...)

tokenize(x, ...)

\method{tokenize}{character}(x, what = c("word", "sentence", "character"),
  cleanFirst = TRUE, verbose = FALSE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, simplify = FALSE, cores = parallel::detectCores(),
  ...)
}
\arguments{
\item{x}{The text(s) or corpus to be tokenized}

\item{...}{additional arguments not used}

\item{what}{the unit for splitting the text, defaults to \code{"word"}.
Available alternatives are \code{c("character", "word", "line_break",
"sentence")}. See \link[stringi]{stringi-search-boundaries}.}

\item{cleanFirst}{clean before tokenizing, if TRUE.  Added for performance
testing only -- we strongly recommend that you NOT use this argument, as we
will remove it from the function soon.}

\item{verbose}{if \code{TRUE}, print timing messages to the console; off by
default}

\item{removeNumbers}{remove tokens that consist only of numbers, but not
words that start with digits, e.g. \code{2day}}

\item{removePunct}{remove all punctuation}

\item{removeSeparators}{remove Separators and separator characters (spaces
and variations of spaces, plus tab, newlines, and anything else in the
Unicode "separator" category) when \code{removePunct=FALSE}}

\item{removeTwitter}{remove Twitter characters \code{@} and \code{#}; set to
\code{FALSE} if you wish to preserve these}

\item{simplify}{if \code{TRUE}, return a character vector of tokens rather
than a list of length \code{\link{ndoc}(texts)}, with each element of the
list containing a character vector of the tokens corresponding to that
text.}

\item{sep}{by default, tokenize expects a "white-space" delimiter between
tokens. Alternatively, \code{sep} can be used to specify another character
which delimits fields.}
}
\value{
A list of length \code{\link{ndoc}(x)} of the tokens found in each text.
}
\description{
Tokenize the texts from a character vector or from a corpus.
}
\examples{
# same for character vectors and for lists
tokensFromChar <- tokenize(inaugTexts[1:3])
tokensFromCorp <- tokenize(subset(inaugCorpus, Year<1798))
identical(tokensFromChar, tokensFromCorp)
str(tokensFromChar)
# returned as a list
head(tokenize(inaugTexts[57])[[1]], 10)
# returned as a character vector using simplify=TRUE
head(tokenize(inaugTexts[57], simplify=TRUE), 10)

# keeping punctuation marks
head(tokenize(inaugTexts[57], simplify=TRUE, removePunct=FALSE), 30)
# keeping case
head(tokenize(inaugTexts[57], simplify=TRUE, toLower=FALSE), 30)

## MORE COMPARISONS
tokenize("#textanalysis is MY <3 4U @myhandle gr8 #stuff :-)", removeTwitter=TRUE)
tokenize("#textanalysis is MY <3 4U @myhandle gr8 #stuff :-)", removeTwitter=FALSE)
#tokenize("great website http://textasdata.com", removeURL=FALSE)
#tokenize("great website http://textasdata.com", removeURL=TRUE)

txt <- c(text1="This is â‚¬10 in 999 different ways,\\n up and down; left and right!",
         text2="@kenbenoit working: on #quanteda 2day\\t4ever, http://textasdata.com?page=123.")
tokenize(txt, verbose=TRUE)
tokenize(txt, removeNumbers=TRUE, removePunct=TRUE)
tokenize(txt, removeNumbers=FALSE, removePunct=TRUE)
tokenize(txt, removeNumbers=TRUE, removePunct=FALSE)
tokenize(txt, removeNumbers=FALSE, removePunct=FALSE)
tokenize(txt, removeNumbers=FALSE, removePunct=FALSE, removeSeparators=FALSE)

# character-level
tokenize("Great website: http://textasdata.com?page=123.", what="character")
tokenize("Great website: http://textasdata.com?page=123.", what="character",
         removeSeparators=FALSE)
}
\author{
Ken Benoit and Paul Nulty
}

