% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/tokenize.R
\name{tokenizeOld.corpus}
\alias{tokenise}
\alias{tokenize}
\alias{tokenize.character}
\alias{tokenizeOld.corpus}
\title{tokenize a set of texts}
\usage{
\method{tokenizeOld}{corpus}(x, ...)

tokenize(x, ...)

\method{tokenize}{character}(x, what = c("word", "sentence", "character"),
  cleanFirst = TRUE, verbose = FALSE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeWhiteSpace = TRUE,
  removeURL = FALSE, simplify = FALSE)
}
\arguments{
\item{x}{The text(s) or corpus to be tokenized}

\item{...}{additional arguments not used}

\item{what}{the unit for splitting the text, defaults to \code{"word"}.
Available alternatives are \code{c("character", "word", "line_break",
"sentence")}. See \link[stringi]{stringi-search-boundaries}.}

\item{cleanFirst}{clean before tokenizing, if TRUE.  Added for performance
testing only -- we strongly recommend that you NOT use this argument, as we
will remove it from the function soon.}

\item{verbose}{if \code{TRUE}, print timing messages to the console; off by
default}

\item{removePunct}{Remove all punctuation}

\item{simplify}{If \code{TRUE}, return a character vector of tokens rather
than a list of length \code{\link{ndoc}(texts)}, with each element of the
list containing a character vector of the tokens corresponding to that
text.}

\item{sep}{by default, tokenize expects a "white-space" delimiter between
tokens. Alternatively, \code{sep} can be used to specify another character
which delimits fields.}

\item{removeDigits}{Remove tokens that consist only of numbers, but not words
that start with digits, e.g. \code{2day}}
}
\value{
A list of length \code{\link{ndoc}(x)} of the tokens found in each text.
}
\description{
Tokenize the texts from a character vector or from a corpus.
}
\examples{
# same for character vectors and for lists
tokensFromChar <- tokenize(inaugTexts[1:3])
tokensFromCorp <- tokenize(subset(inaugCorpus, Year<1798))
identical(tokensFromChar, tokensFromCorp)
str(tokensFromChar)
# returned as a list
head(tokenize(inaugTexts[57])[[1]], 10)
# returned as a character vector using simplify=TRUE
head(tokenize(inaugTexts[57], simplify=TRUE), 10)

# demonstrate some options with clean
head(tokenize(inaugTexts[57], simplify=TRUE, removePunct=FALSE), 30)

## MORE COMPARISONS
tokenize("this is MY <3 4U @myhandle gr8 stuff :-)", removeTwitter=TRUE)
tokenize("this is MY <3 4U @myhandle gr8 stuff :-)", removeTwitter=FALSE)
tokenize("great website http://textasdata.com", removeURL=FALSE)
tokenize("great website http://textasdata.com", removeURL=TRUE)
txt <- c(text1="This is â‚¬10 in 999 different ways,\\n up and down; left and right!",
         text2="@kenbenoit working: on #quanteda 2day and\\t4ever.")
tokenize(txt)
tokenize(txt, removeDigits=TRUE, removePunct=TRUE)
tokenize(txt, removeDigits=FALSE, removePunct=TRUE)
tokenize(txt, removeDigits=FALSE, removePunct=FALSE)
}
\author{
Ken Benoit and Paul Nulty
}

