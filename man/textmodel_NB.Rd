% Generated by roxygen2 (4.0.2): do not edit by hand
\name{textmodel_NB}
\alias{textmodel_NB}
\title{Naive Bayes text model}
\usage{
textmodel_NB(x, train, train.class, smooth = 1,
  distribution = c("multinomial", "bernoulli"),
  classpriors = 1/length(table(train.class)))
}
\arguments{
\item{x}{the dfm on which the model will be fit.  Does not need to contain
only the training documents, since the index of these will be identified in
\code{train}.}

\item{train}{integer index of documents, or character vector of document
names.  For \code{scale="logit"}, only two documents can be specified}

\item{train.class}{vector of training labels associated with each document
identified in \code{train}.  (These will be converted to factors if not
already factors.)}

\item{smooth}{a smoothing parameter for word counts, default is 1.0.}

\item{distribution}{either \code{multinomial} to count all word occurrences,
or \code{bernoulli} to count feature occurrences as binary}

\item{classpriors}{an optional named vector of class priors, corresponding to
the levels or values of \code{train.class}.  This does not need to be
ordered, but must be named, and the names must match 1:1 to the levels of
\code{train.class}.  Default is uniform class priors, not based on the
balance of trainind documents, but rather uniform across the possible
classes.  See details.}
}
\description{
\code{textmodel_NB} implements Naive Bayes model for class prediction on a
set of labelled texts.
}
\section{Class priors}{
 Details on this will be provided soon.
}

\section{Smoothing values}{
 Smoothing values are added to features observed in
  each class, which consist of all observed features per class, not per
  document.  More soon.
}
\author{
Kenneth Benoit
}
\references{
Laver, Benoit and Garry (2003); Martin and Vanberg (2007)
}

