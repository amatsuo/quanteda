% Generated by roxygen2 (4.0.2): do not edit by hand
\name{textmodel_NB}
\alias{textmodel_NB}
\title{Naive Bayes classifier for texts}
\usage{
textmodel_NB(x, y, smooth = 1, prior = "uniform",
  distribution = "multinomial", ...)
}
\arguments{
\item{x}{the dfm on which the model will be fit.  Does not need to contain
only the training documents.}

\item{y}{vector of training labels associated with each document
identified in \code{train}.  (These will be converted to factors if not
already factors.)}

\item{smooth}{smoothing parameter for feature counts by class}

\item{prior}{prior distribution on texts, see details}

\item{distribution}{count model for text features, can be \code{multinomial}
or \code{Bernoulli}}

\item{...}{more arguments passed through}
}
\value{
A list of return values, consisting of:

\item{call}{original function call}

\item{PwGc}{probability of the word given the class (empirical
  likelihood)}

\item{Pc}{class prior probability}

\item{PcGw}{posterior class probability given the word}

\item{Pw}{baseline probability of the word}

\item{data}{list consisting of \code{x} training class, and \code{y}
  test class}

\item{distribution}{the distribution argument}

\item{prior}{argument passed as a prior}

\item{smooth}{smoothing parameter}
}
\description{
Currently working for vectors of texts -- not specially defined for a dfm.
}
\details{
This naive Bayes model works on word counts, with smoothing.
}
\author{
Kenneth Benoit
}

