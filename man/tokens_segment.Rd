% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokens_segment.R
\name{tokens_segment}
\alias{tokens_segment}
\title{segment tokens object by patterns}
\usage{
tokens_segment(x, what = c("sentences", "other"), delimiter = NULL,
  valuetype = c("glob", "regex", "fixed"), case_insensitive = TRUE,
  remove_delimiter = FALSE, verbose = quanteda_options("verbose"))
}
\arguments{
\item{x}{\link{tokens} object whose token elements will be segmented}

\item{what}{unit of segmentation.  Current options are
\code{"sentences"} (default) and \code{"other"}.

Segmenting on \code{"other"} allows segmentation of a text on any 
user-defined value, and must be accompanied by the \code{delimiter} 
argument.}

\item{delimiter}{the same as \code{pattern}}

\item{valuetype}{how to interpret keyword expressions: \code{"glob"} for 
"glob"-style wildcard expressions; \code{"regex"} for regular expressions;
or \code{"fixed"} for exact matching. See \link{valuetype} for details.}

\item{case_insensitive}{ignore case when matching, if \code{TRUE}}

\item{remove_delimiter}{remove deimiter, if \code{TRUE}}

\item{verbose}{if \code{TRUE} print messages about how many tokens were 
selected or removed}
}
\value{
@return \code{tokens_segment} returns a tokens of segmented texts
}
\description{
segment tokens object by patterns
}
\examples{
txts <- "Fellow citizens, I am again called upon by the voice of my country to
execute the functions of its Chief Magistrate. When the occasion proper for
it shall arrive, I shall endeavor to express the high sense I entertain of
this distinguished honor."
toks <- tokens(txts)

# split into sentences
toks_sent <- tokens_segment(toks, what = "sentences")

# split by any punctuation
toks_punc <- tokens_segment(toks, what = "other", delimiter = "[\\\\p{P}]", valuetype = 'regex',
                            remove_delimiter = FALSE)
toks_punc <- tokens_segment(toks, what = "other", delimiter = "[\\\\p{P}]", valuetype = 'regex', 
                            remove_delimiter = TRUE)

}
\keyword{internal}
\keyword{tokens}
