---
title: "Example: fcm for word embedding "
author: Kenneth Benoit, Haiyan Wang and Kohei Watanabe
output: 
  rmarkdown::html_vignette:
    toc: yes
---
```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "##")
```

```{r, message = FALSE}
library(quanteda)
library(text2vec)
```

This vignette provides a basic overview on how to apply **quanteda**'s feature co-occurrence matrix for **GloVe** word embedding.  

###Read the corpus 
from the [**text2vec** vignette](http://text2vec.org/glove.html):
```{r eval = TRUE}
temp_file <- '/tmp/wiki.RDS'
if (!file.exists(temp_file)) {
    wiki <- corpus(readtext::readtext("http://mattmahoney.net/dc/text8.zip", verbosity = 0))
    saveRDS(wiki, file = paste0(temp_file))
} else {
    wiki <- readRDS(paste0(temp_file))
}
```

##Trim the features before constructing the fcm:
```{r}
wiki_dfm <- dfm(wiki, verbose = TRUE)
feat <- featnames(dfm_trim(wiki_dfm, min_count = 5, verbose = TRUE))
wiki_toks <- tokens(wiki) %>% tokens_select(feat, padding = TRUE)
```

##Constructing the feature co-occurrence matrix(fcm):
```{r}
wiki_fcm <- fcm(wiki_toks, context = "window", count = "weighted", weights = 1/(1:5), tri = TRUE)
```

## Fit the [GloVe model](https://nlp.stanford.edu/pubs/glove.pdf) using package [text2vec](http://text2vec.org)
```{r}
glove = GlobalVectors$new(word_vectors_size = 50, vocabulary = types(wiki_toks), x_max = 10)
wiki_main = fit_transform(wiki_fcm, glove, n_iter = 20)
```

##The Glove model leans two sets of word vectors 
The two vectors are main and context. According to Glove paper, averaging the two word vectors results in more accurate representation.
```{r}
wiki_context = glove$components
dim(wiki_context)

wiki_vectors = wiki_main + t(wiki_context)
```

## Now we can find the closest word vectors for `paris - france + germany`
```{r}
berlin <-  wiki_vectors["paris", , drop = FALSE] - 
  wiki_vectors["france", , drop = FALSE] + 
  wiki_vectors["germany", , drop = FALSE]

# calculate the similarity
wiki_vector_dfm <- as.dfm(rbind(wiki_vectors, berlin))
wiki_vector_dfm@Dimnames$docs[dim(wiki_vector_dfm)[1]] <- "new_berlin"
cos_sim <-  textstat_simil(wiki_vector_dfm, "new_berlin", 
                           margin = "documents", method= "cosine")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```

## Here is another example for `london = paris - france + uk + england`
```{r}
london <-  wiki_vectors["paris", , drop = FALSE] - 
    wiki_vectors["france", , drop = FALSE] + 
    wiki_vectors["uk", , drop = FALSE] + 
    wiki_vectors["england", , drop = FALSE] 

wiki_vector_dfm <- as.dfm(rbind(wiki_vectors, london))
wiki_vector_dfm@Dimnames$docs[dim(wiki_vector_dfm)[1]] <- "new_london"
cos_sim <-  textstat_simil(wiki_vector_dfm, "new_london", 
                           margin = "documents", method= "cosine")
```
