---
title: "Example: word embedding (word2vec)"
author: Kenneth Benoit, Haiyan Wang and Kohei Watanabe
output: 
  rmarkdown::html_vignette:
    toc: no
---
    
```{r, message = FALSE}
library(quanteda)
library(text2vec)
```

###Reading the corpus from the [**text2vec** vignette](http://text2vec.org/glove.html):
```{r eval = TRUE}
temp_file <- '/tmp/wiki.RDS'
if (!file.exists(temp_file)) {
    wiki <- corpus(readtext::readtext("http://mattmahoney.net/dc/text8.zip", verbosity = 0))
    saveRDS(wiki, file = paste0(temp_file))
} else {
    wiki <- readRDS(paste0(temp_file))
}
```

###Trimming the features before constructing the fcm:
```{r}
wiki_dfm <- dfm(wiki, verbose = TRUE)
feat <- featnames(dfm_trim(wiki_dfm, min_count = 5, verbose = TRUE))
wiki_toks <- tokens(wiki) %>% tokens_select(feat, padding = TRUE)
```

###Constructing the feature co-occurrence matrix(fcm):
```{r}
wiki_fcm <- fcm(wiki_toks, context = "window", count = "weighted", weights = 1/(1:5), tri = TRUE)
```

### Fit the (GloVe model)[https://nlp.stanford.edu/pubs/glove.pdf]
```{r}
glove = GlobalVectors$new(word_vectors_size = 50, vocabulary = qvocab, x_max = 10)
# `glove` object will be modified by `fit_transform()` call !
wiki_main = fit_transform(wiki_fcm, glove, n_iter = 20)

wiki_context = glove$components
dim(wiki_context)

wiki_vectors = wiki_main + t(wiki_context)

berlin = wiki_vectors["paris", , drop = FALSE] - 
  wiki_vectors["france", , drop = FALSE] + 
  wiki_vectors["germany", , drop = FALSE]

wiki_vector_dfm <- as.dfm(rbind(wiki_vectors, berlin))
wiki_vector_dfm@Dimnames$docs[dim(wiki_vector_dfm)[1]] <- "new_berlin"
cos_sim = textstat_simil(wiki_vector_dfm, "new_berlin", margin = "documents", method= "cosine")
head(sort(cos_sim[,1], decreasing = TRUE), 5)

```

