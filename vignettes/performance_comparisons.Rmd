---
title: "quanteda Performance Comparisons and Tests"
author: "Ken Benoit and Paul Nulty"
date: "2015-06-22"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Development Plans}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

## Tokenization


### Example: Kohei's news texts

```{r}
require(quanteda)
koheiCorpus <- corpus(textfile("~/Dropbox/QUANTESS/Testing/Kohei tokenizer/1996-1997.txt"))
koheiCorpus
nchar(texts(koheiCorpus))
```


**quanteda** tokenize in `devWorking`:
```{r}
# defaults, including lowercase and normal cleaning
system.time(tokenize(texts(koheiCorpus)))
# just tokenization
system.time(tokenize(texts(koheiCorpus), toLower=FALSE, removeNumbers=FALSE, removePunct=FALSE, removeSeparators=FALSE))
```

Tokenization only, two **stringi** methods:
```{r}
# fixed on a space
system.time(stringi::stri_split_fixed(toLower(texts(koheiCorpus)), " ", simplify=FALSE))
# smarter method with whitespace class
system.time(stringi::stri_split_regex(toLower(texts(koheiCorpus)), "\\s", simplify=FALSE))
```
Note that in these, there is *no lowercasing*, *no removal of numbers or punctuation*.

For comparison with **tm**'s fastest tokenizer (using `scan()`):
```{r}
system.time(tm::scan_tokenizer(toLower(texts(koheiCorpus))))
```

### Conclusion

The fastest *usable* method is the `stri_split_regex` to split on `"\\s"`, and this is just about **2 seconds** slower than the full monty `tokenize()` default.  In that 2 seconds, I seriously doubt we can perform the removal of numbers and punctuation that are part of the `tokenize()` default of `stri_split_boundaries(, type="word", skip_word_none = removePunct, skip_word_number = removeNumbers)`.  And: The ICU boundary recognition is smarter than splitting on `\\s`. 

Conclusion: We stick with `tokenize()` in its current form.