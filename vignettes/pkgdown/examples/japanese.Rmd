---
title: "事例: 衆議院外務委員会の議事録"
author: Kohei Watanabe and Akitaka Matsuo
output: 
  html_document:
    toc: true
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "##"
)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## パッケージの読み込み
```{r, message = FALSE}
devtools::install_github("quanteda/quanteda.data")
library(quanteda)
library(quanteda.data)
library(dplyr)
library(stringr)
library(lubridate)
```


## 衆議院外務委員会議事録のコーパスをダウンロード
国会会議録のダウンロードには`kaigiroku`パッケージを使う。
```{r eval = FALSE}
devtools::install_github("amatsuo/kaigiroku")
library(kaigiroku)

tmp_dl_folder <- "~/Desktop/temp/foreign_affairs_committee/"
# ここでは年ごとのスピーチデータを作成してそれをあとで連結している
# APIが応答しない場合に途中からダウンロードをやり直す必要があるため
current_committee <- "外務"
for(year in c(1947:2017)) {
  cat(paste0(Sys.time() %>% as.character(), " ", year, current_committee, "\n"))
  speech_data <- get_meeting(meetingName = current_committee, year = year)
  if(is.null(speech_data)) next
  outfile <- sprintf("%s/%s_%s.Rdata", tmp_dl_folder, year, current_committee)
  save(speech_data, file = outfile, compress = "bzip2")
  Sys.sleep(10)
}

files <- list.files(tmp_dl_folder, 
                    full = TRUE, pattern = "Rdata")

foreign_affairs_committee_speeches <- 
  lapply(as.list(files), function(x){load(x); return(speech_data)}) %>%
  bind_rows()
save(foreign_affairs_committee_speeches, 
     file = paste0(tmp_dl_folder, "foreign_affairs_committee_speeches.rda"))
```

### データのロード、quantedaコーパスの作成
```{r}
data_folder <- "~/Dropbox/temp/"

load(paste0(data_folder, "foreign_affairs_committee_speeches.rda"))
## 不要な変数を消去、変数をリネーム（テキストフィールドの変数名を"text"としておくと
## corpus()は自動で判別し、"text"からコーパスを作成し、
## 残りの変数をdocvarに格納してくれる
foreign_affairs_committee_speeches <- 
  foreign_affairs_committee_speeches %>% 
  select(-c(meetingURL, pdfURL)) %>% 
  rename(text = speech)
  
corp_full <- corpus(foreign_affairs_committee_speeches)

```


## 議事録の数と期間を取得
```{r}
ndoc(corp_full)
range(docvars(corp_full, 'date'))
```



# 前処理
発言者のいないレコード（典型的には各議事録の0番目の出席者、議題等の部分）を取り除く、また、各発言の冒頭は発言者の名前＋役職名なので、その部分から役職を取り出して新しい`docvar`を作る。

```{r}
corp_full <- corpus_subset(corp_full, speaker != "")

## capacity変数の作成
capacity <- corp_full %>%
  str_sub(1, 20) %>%
  str_replace_all("\\s+.+|\n", "") %>% # 冒頭の名前部分の取り出し
  str_replace( "^.+?(参事|政府特別補佐人|内閣官房|会計検査院|最高裁判所長官代理者|主査|議員|副?大臣|副?議長|委員|参考人|分科員|公述人|君(（.+）)?$)", "\\1") %>% # 冒頭の○から、名前部分までを消去
  str_replace("（.+）", "")
capacity <- str_replace(capacity, "^○.+", "Other") # マイナーな役職名は一括して"Other"に
knitr::kable(as.data.frame(table(capacity)))

docvars(corp_full, 'capacity') <- capacity
```

## 1991から2010年までの期間の議事録を選択
```{r}
docvars(corp_full, "year") <- docvars(corp_full, "date") %>% year() %>% as.numeric()
corp <- corpus_subset(corp_full, 1991 <= year & year <= 2010)

ndoc(corp)
```

## 委員、大臣、副大臣の発言を選択
```{r}
corp <- corpus_subset(corp, capacity %in% c("委員", "大臣", "副大臣"))
ndoc(corp)
```


## 語の分割と結合
日本語の分析では，形態素解析ツールを用いて分かち書きを行うことが多いが，**quanteda**の`tokens()`は，[ICU](http://site.icu-project.org/)で定義された規則に従って文を語に分割することができる．さらに，漢字やカタカナの連続的共起を`textstat_collocations()`を用いて抽出し，`tokens_compound()`によって統計的に優位な組み合わせを結合すると，より質の高いトークン化を実現できる．`textstat_collocations()`を用いる場合は，事前に`tokens_select()`と正規表現で，対象とする語だけを選択する．この際，`padding = TRUE`とし，語の間の距離が維持されるように注意する．
```{r}
toks <- tokens(corp)
toks <- tokens_select(toks, '^[０-９ぁ-んァ-ヶー一-龠]+$', valuetype = 'regex', padding = TRUE)
toks <- tokens_remove(toks, c('御', '君'), padding = TRUE)

min_count <- 10

# 漢字
toks_kanji <- tokens_select(toks, '^[一-龠]+$', valuetype = 'regex', padding = TRUE)
col_kanji <- textstat_collocations(toks_kanji, min_count = min_count)
toks <- tokens_compound(toks, col_kanji[col_kanji$z > 3,], concatenator = '')

# カタカナ
toks_kana <- tokens_select(toks, '^[ァ-ヶー]+$', valuetype = 'regex', padding = TRUE)
col_kana <- textstat_collocations(toks_kana, min_count = min_count)
toks <- tokens_compound(toks, col_kana[col_kana$z > 3,], concatenator = '')

# 漢字，カタカナおよび数字
toks_any <- tokens_select(toks, '^[０-９ァ-ヶー一-龠]+$', valuetype = 'regex', padding = TRUE)
col_any <- textstat_collocations(toks_any, min_count = min_count)
toks <- tokens_compound(toks, col_any[col_any$z > 3,], concatenator = '')
```

## 文書行列の作成
`dfm()`によって文書行列を作成した後でも，`dfm_*()`と命名された関数を用いると分析に必要な文書の特徴を自由に選択できる．ここでは，平仮名のみもしくは一語のみから構成されたトークンを`dfm_remove()`によって，頻度が極端に低い語もしくは高い語を`dfm_trim()`によって削除している．
```{r}
mt <- dfm(toks, remove = '')
mt <- dfm_remove(mt, '^[ぁ-ん]+$', valuetype = 'regex', min_nchar = 2)
mt <- dfm_trim(mt, min_count = 10, max_count = 0.1)

```

# 分析

## Keynessによる相対頻度分析

`textstat_keyness()`は語の頻度を文書のグループ間で比較し，統計的に有意に頻度が高いものを選択する．ここでは，同時多発テロが発生した2001年以降に頻度が高くなった30語を示してある．

```{r}
key <- textstat_keyness(mt, docvars(mt, 'year') >= 2001)
knitr::kable(head(key, 20))
```

上の表では，委員会出席者の名前が多く含まれるので，それらを取り除くと議論の主題が明確になる．

```{r}
key <- key[!stringi::stri_detect_regex(rownames(key), '委員|大臣'),]
knitr::kable(head(key, 20))
```


## 共起ネットワーク分析による視覚化
`fcm()`によって作成した共起行列に対して，`textplot_network()`を用いると語の関係が視覚化でき，文書の内容の全体像を容易に把握できる．
```{r}
feat <- head(rownames(key), 100)
mt_co <- fcm(dfm_select(mt, feat))
size <- sqrt(rowSums(mt_co))
textplot_network(mt_co, min_freq = 0.95, edge_alpha = 0.9, vertex_size = size / max(size) * 3)
```

## トピックモデルの推定
`convert()`を利用して、データ形式を変換し、`topicmodels::LDA`で推定する。
```{r eval = FALSE}
require(topicmodels)

model_lda_foreign_affairs <- LDA(convert(mt, to = "topicmodels"), k = 10)
get_terms(model_lda_foreign_affairs, 10)
```
