---
title: "Quick Start Guide"
output: 
  html_document:
    toc: true
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = FALSE, 
                      comment = "##")
```

このビネットでは、**quanteda**の特徴と機能の基本的な概要を解説します。 追加のビネットについては、[ quanteda.ioにあるその他の記事](http://docs.quanteda.io/articles/index.html)を参照してください。

# パッケージのインストール

**quanteda**は[CRAN](https://CRAN.R-project.org/package=quanteda)からインストールできます。GUIのRパッケージインストーラを使用してインストールするか、次のコマンドを実行します。

```{r, eval = FALSE}
install.packages("quanteda") 
```

GitHubから最新の開発バージョンをインストールする方法については、https://github.com/kbenoit/quanteda を参照してください。

## インストールが推奨されるパッケージ

**quanteda**には**quanteda**と連携して機能を拡張する、一連のパッケージがあります。それらもインストールすることを推奨します：

* [**readtext**](https://github.com/kbenoit/readtext)：多くの入力形式からテキストデータをRに簡単に読み込むパッケージ。
* [**spacyr**](https://github.com/kbenoit/spacyr)：pythonの[spaCy](http://spacy.io)ライブラリを使用した自然言語解析のためのパッケージ。品詞タグ付け、固有表現抽出、および係り受け関係の解析など。
* [**quantedaData**](https://github.com/kbenoit/quantedaData)：**quanteda**のビネット等で使用する追加のテキストデータ。

    ```{r eval = FALSE}
    devtools::install_github("kbenoit/quantedaData")
    ```
*  [**LIWCalike**](https://github.com/kbenoit/LIWCalike): [Linguistic Inquiry and Word Count](http://liwc.wpengine.com) (LIWC) アプローチによるテキスト分析のR実装.
    ```{r eval = FALSE}
    devtools::install_github("kbenoit/LIWCalike")
    ```

# コーパスの作成

まず、パッケージをロードして、パッケージ内の関数とデータにアクセスできるようにします。

```{r, message = FALSE}
library(quanteda)
```

## 利用可能なコーパス

**quanteda**にはテキストを読み込むためのシンプルで強力なコンパニオンパッケージ、[**readtext**](https://github.com/kbenoit/readtext)があります。
このパッケージのmain関数`readtext()`は、ディスクやURLからファイルやファイルセットを取り出し、**quanteda**の`corpus()`コンストラクタ関数から直接呼び出せる形式の、data.frameを返します。

`readtext()`で利用可能なファイルやデータの形式:

* テキスト(`.txt`)ファイル
* コンマ区切り値(`.csv`)ファイル
* XML形式のデータ
* JSON形式のFacebook APIのデータ
* JSON形式のTwitter APIのデータ
* より一般的なJSONデータ

**quanteda**のコーパスコンストラクタコマンド `corpus()`は、次のクラスのオブジェクトを受け付けます：

* キャラクタオブジェクトのベクトル(例:他のツールを使用してすでにワークスペースにロードしたテキスト)
* **tm**パッケージの `VCorpus`コーパスオブジェクト
* テキスト列と他のドキュメントレベルのメタデータを含むdata.frame

### 例：文字列からコーパスを作成

最も単純なケースは、すでにRのメモリにあるテキストのベクトルからコーパスを作成することです。テキストのベクトルをRに取り込む方法は実にたくさんあるので、高度なRユーザーは、入力するテキストをいろいろな方法で作り出せます。

すでにテキストベクトル形式のテキストがある場合は、コーパスコンストラクタ関数を直接呼び出すことができます。 次の例では、**quanteda**パッケージと一緒に配布される、イギリスの政党の2010年の選挙マニフェストから抽出されたテキストオブジェクト(`data_char_ukimmig2010`)をつかって、コーパスを作成しています。

```{r}
myCorpus <- corpus(data_char_ukimmig2010)  # テキストからコーパスを作成
summary(myCorpus)
```

コーパスを作成したあとで、もし必要ならば、ドキュメントレベルの変数(`docvars`と呼ばれるもの)をこのコーパスに追加することができます。

たとえば、Rの`names()`関数を使って文字ベクトル`data_char_ukimmig2010`の名前を取得し、これをドキュメント変数(`docvar`)に追加することができます。

```{r}
docvars(myCorpus, "Party") <- names(data_char_ukimmig2010)
docvars(myCorpus, "Year") <- 2010
summary(myCorpus)
```

分析の対象となるドキュメント変数では必ずしもないけれども、ドキュメントの属性として残しておきたいと思う追加のメタデータも、`docvars()`を使って、コーパスに追加することができます。

```{r}
metadoc(myCorpus, "language") <- "english"
metadoc(myCorpus, "docsource")  <- paste("data_char_ukimmig2010", 1:ndoc(myCorpus), sep = "_")
summary(myCorpus, showmeta = TRUE)
```

このコマンド `metadoc`では、ドキュメントメタデータフィールドを自由に定義することができます。 `language`フィールドに、単一の値`english`をアサインするときには、Rは値を再利用してステベのドキュメントにアサインしていることに注意してください。

独自のmetadocフィールド`docsource`のタグを作成する際には、quanteda関数`ndoc()`を使ってコーパスの文書数を取り出しました。この関数`ndoc()`は、`nrow()`や `ncol()`のようなR標準の関数と同じような方法で動作するように意図して設計されています。

### 例：**readtext**パッケージを用いたファイルの読み込み

```{r, eval=FALSE}
require(readtext)

# Twitter json
mytf1 <- readtext("~/Dropbox/QUANTESS/social media/zombies/tweets.json")
myCorpusTwitter <- corpus(mytf1)
summary(myCorpusTwitter, 5)
# generic json - needs a textfield specifier
mytf2 <- readtext("~/Dropbox/QUANTESS/Manuscripts/collocations/Corpora/sotu/sotu.json",
                  textfield = "text")
summary(corpus(mytf2), 5)
# text file
mytf3 <- readtext("~/Dropbox/QUANTESS/corpora/project_gutenberg/pg2701.txt", cache = FALSE)
summary(corpus(mytf3), 5)
# multiple text files
mytf4 <- readtext("~/Dropbox/QUANTESS/corpora/inaugural/*.txt", cache = FALSE)
summary(corpus(mytf4), 5)
# multiple text files with docvars from filenames
mytf5 <- readtext("~/Dropbox/QUANTESS/corpora/inaugural/*.txt", 
                  docvarsfrom = "filenames", sep = "-", docvarnames = c("Year", "President"))
summary(corpus(mytf5), 5)
# XML data
mytf6 <- readtext("~/Dropbox/QUANTESS/quanteda_working_files/xmlData/plant_catalog.xml", 
                  textfield = "COMMON")
summary(corpus(mytf6), 5)
# csv file
write.csv(data.frame(inaugSpeech = texts(data_corpus_inaugural), 
                     docvars(data_corpus_inaugural)),
          file = "/tmp/inaug_texts.csv", row.names = FALSE)
mytf7 <- readtext("/tmp/inaug_texts.csv", textfield = "inaugSpeech")
summary(corpus(mytf7), 5)
```

## コーパスオブジェクトの使い方

### コーパスの原理

コーパスを設計するのにあたりquantedaでは、元になったドキュメントをプレーンなUTF-8でエンコードされたテキストに変換し、コーパスレベルおよびドキュメントレベルのメタデータと一緒に格納する、ドキュメントの「ライブラリ」としてコーパスが機能するようにしました。ドキュメントレベルのメタデータには*docvars*という名前がつけられています。*docvars*は、各文書の属性を記述する変数（あるいはフィーチャー）です。

コーパスは基本的には、処理および分析に関して、テキストの静的なコンテナになるように設計されています。これが意味するのは、コーパス内のテキストは、ステミングや句読点の削除などのテキストのクリーニングや前処理ステップによって内部的に変更されるようには設計されていない、ということです。このことで、処理の一部としてコーパスからテキストを抽出して新しいオブジェクトに割り当てた後でも、コーパスは元のテキストの参照コピーとして残り、たとえばステミング前の単語や句読点を必要とする可読性の分析などの分析を、同じコーパス上で実行することができるようになるのです。

コーパスからテキストを抽出するためには、`texts()`と呼ばれる抽出子を使用します。

```{r}
texts(data_corpus_inaugural)[2]
```


`summary()`メソッドにより、コーパス内のテキストの要約を行うことができます。

```{r}
summary(data_corpus_irishbudget2010)
```

summaryコマンドの出力をデータフレームとして保存し、この情報を使って基本的な記述統計をプロットすることができます：

```{r, fig.width = 8}
tokenInfo <- summary(data_corpus_inaugural)
if (require(ggplot2))
    ggplot(data=tokenInfo, aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() +
        scale_x_continuous(labels = c(seq(1789,2012,12)), breaks = seq(1789,2012,12) ) 

# Longest inaugural address: William Henry Harrison
tokenInfo[which.max(tokenInfo$Tokens), ] 
```


## コーパスに対する操作

### コーパスの結合

`+`演算子で、2つのコーパスオブジェクトを簡単に連結できます。ドキュメントレベルの変数が2つのコーパスで異なる場合には、ドキュメント変数は継ぎ接ぎされて、情報が失われることを防ぎます。コーパスレベルのメタデータも結合されます。

```{r}
library(quanteda)
mycorpus1 <- corpus(data_corpus_inaugural[1:5])
mycorpus2 <- corpus(data_corpus_inaugural[53:58])
mycorpus3 <- mycorpus1 + mycorpus2
summary(mycorpus3)
```

### コーパス内の文書の一部の文書の抽出

コーパスオブジェクトに対して定義された `corpus_subset()`関数のメソッドがあります。この関数により、ドキュメント変数
docvarsに適用される論理条件に基づいて文書を抽出します。

```{r}
summary(corpus_subset(data_corpus_inaugural, Year > 1990))
summary(corpus_subset(data_corpus_inaugural, President == "Adams"))
```


## コーパス内の文書の探索

`kwic`(KeyWords-In-Context)関数は単語の検索を行い、その単語が生起する文脈を表示します：

```{r, tidy=TRUE}
kwic(data_corpus_inaugural, "terror")
```

```{r}
kwic(data_corpus_inaugural, "terror", valuetype = "regex")
```

```{r}
kwic(data_corpus_inaugural, "communist*")
```


上記の要約では、「Year」と「President」は各文書に結び付けられた変数です。 `docvars()`関数でこのような変数にアクセスできます。

```{r}
# inspect the document-level variables
head(docvars(data_corpus_inaugural))

# inspect the corpus-level metadata
metacorpus(data_corpus_inaugural)
```

[quantedaData](http://github.com/kbenoit/quantedaData)パッケージをインストールすることにより、もっとたくさんのコーパスを試すことができます。


# コーパスからフィーチャーを抽出

文書のスケーリングなどの統計分析を行うためには、各ドキュメントについて特定のフィーチャーの値を関連付けた行列を抽出する必要があります。 quantedaでは、このような行列を生成するために `dfm`関数を使います。 "dfm"は*document-feature matrix*の略で、各行が一つのドキュメント、各列が「フィーチャー」となる行列です。 行と列をこのように規定する理由は、データ分析では行が分析単位になり、各列が分析対象になる変数となるのが通常だからです。 他のソフトウェアのように、「ターム」という用語を用いず「フィーチャー」という用語を使う理由は、フィーチャーのほうが一般性を持つ用語だからです。様々なものがフィーチャーとなります。元の単語、ステムされた単語、品詞解析された単語、ストップワードが削除された後の単語、あるいは、辞書分析によりその単語が属するとされたクラス、などです。 更にはngramsや構文の係り受けの関係などをフィーチャーとするなどいろいろな可能性がありえ、`quanteda`は何がフィーチャーになるかを細かく規定せずオープンに受け入れます。

## 文書のトークン化

テキストを簡単にトークン化するために、quantedaは `tokens()`と呼ばれる強力なコマンドを提供します。 この関数は、文字ベクトルのトークンのリストからなる中間オブジェクトを生成します。ここで、リストの一つ一つの要素は入力されたドキュメントに対応しています。

`tokens()`は意図して保守的に設計されています。ユーザーが明示的に指示を与えないかぎりは、要素を削除しません。

```{r}
txt <- c(text1 = "This is $10 in 999 different ways,\n up and down; left and right!", 
         text2 = "@kenbenoit working: on #quanteda 2day\t4ever, http://textasdata.com?page=123.")
tokens(txt)
tokens(txt, remove_numbers = TRUE,  remove_punct = TRUE)
tokens(txt, remove_numbers = FALSE, remove_punct = TRUE)
tokens(txt, remove_numbers = TRUE,  remove_punct = FALSE)
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE)
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE, remove_separators = FALSE)
```

`tokens`には個々の文字をトークン化するオプションもあります:
```{r}
tokens("Great website: http://textasdata.com?page=123.", what = "character")
tokens("Great website: http://textasdata.com?page=123.", what = "character", 
         remove_separators = FALSE)
```

もしくは、一文ごとにトークン化するオプションもあります。
```{r}
# sentence level         
tokens(c("Kurt Vongeut said; only assholes use semi-colons.", 
           "Today is Thursday in Canberra:  It is yesterday in London.", 
           "En el caso de que no puedas ir con ellos, ¿quieres ir con nosotros?"), 
          what = "sentence")
```

## ドキュメント-フィーチャー行列の作成

テキストをトークン化することは中間的な選択肢であり、ほとんどのユーザーはこれをスキップしてドキュメント-フィーチャー行列を作成したいと考えるでしょう。 このために、quantedaはワンステップで、トークン化を実行し、抽出されたフィーチャーをドキュメントｘフィーチャーのマトリックスに集計する `dfm()`と呼ばれるスイスアーミーナイフのように便利な関数を持っています。 `tokens()`の保守的なアプローチとは異なり、 `dfm()`関数はデフォルトで `tolower`（大文字を小文字へと置き換える独立の関数）や句読点を取り除くなどのオプションを適用します。  `tokens()`のオプションはすべて `dfm()`に渡すことができます。

```{r}
myCorpus <- corpus_subset(data_corpus_inaugural, Year > 1990)

# make a dfm
myDfm <- dfm(myCorpus)
myDfm[, 1:5]
```

`dfm()`のその他のオプションには、ストップワードの削除やトークンのステミングが含まれます。

```{r}
# make a dfm, removing stopwords and applying stemming
myStemMat <- dfm(myCorpus, remove = stopwords("english"), 
                 stem = TRUE, remove_punct = TRUE)
myStemMat[, 1:5]
```


`remove`オプションは、ドキュメント-フィーチャー行列から除外するトークンのリストを提供します。 多くのユーザーはすでに定義された「ストップワード」のリストを使うことになるでしょう。

`stopwords()`関数は、幾つかの言語で定義された、ストップワードの辞書を提供します（残念ながら日本語は含まれていませんが、ユーザーが定義した辞書を使うことはできます）。

```{r}
head(stopwords("english"), 20)
head(stopwords("russian"), 10)
head(stopwords("arabic"), 10)
```

### 文書行列の表示

dfmは、RStudioのEnvironmentペインで、またはRの `View`関数を呼び出すことで調べることができます。 dfmで `plot`を呼び出すと、[wordcloudパッケージ](https://cran.r-project.org/web/packages/wordcloud)を使ってワードクラウドが表示されます。

```{r warning=FALSE, fig.width = 8, fig.height = 8}
mydfm <- dfm(data_char_ukimmig2010, remove = stopwords("english"), remove_punct = TRUE)
mydfm
```

頻度が最も高いフィーチャーを見るには、`topfeatures()`を使います:
```{r}
topfeatures(mydfm, 20)  # 20 top words
```

`dfm`オブジェクトを`textplot_wordcloud()`に渡すことで、ワードクラウドをプロットできます。この関数は、オブジェクトや引数を**wordcloud**パッケージの`wordcloud()`関数に送るので、ワードクラウドのプロットを変更することもできます。
```{r warning=FALSE, fig.width = 7, fig.height = 7}
set.seed(100)
textplot_wordcloud(mydfm, min.freq = 6, random.order = FALSE,
                   rot.per = .25, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"))
```

### ドキュメント変数による文書のグループ化

多くの場合で、ドキュメントごとではなく、ドキュメント変数の値によってグループを作って分析したい、と思うこともあるでしょう。quantedaでは、dfmを作るときに、ドキュメント変数のそれぞれの値ごとにドキュメントをグループ化するすることができます。

```{r}
byPartyDfm <- dfm(data_corpus_irishbudget2010, groups = "party", 
                  remove = stopwords("english"), remove_punct = TRUE)
```

以下のように、dfmをソートして、中身を確かめられます。
```{r}
dfm_sort(byPartyDfm)[, 1:10]
```

この例で、最頻出のフィーチャーは、多くの場合英語のストップリストにある単語である「will」ですが、
quantedaの組み込み英語ストップワードリストには、この単語は含まれていません。

### 辞書を用いた語のグループ化

テキストから測定して指標化したい特徴的な単語の集合についての事前知識を持っている場合を考えてみましょう。 たとえば、映画レビューで好意的な感情に関連するような、好意的単語の辞書をすでに持っている、または特定のイデオロギー的なスタンスに関連する政治用語の辞書を持っているというような場合です。 このような場合、これらの単語のグループをまとめて一つのものとして取り扱い、これらの単語を合計して一つのクラスとしてカウントして分析すると上手くいくかもしれません。

次の例では、テロリズムに関連する言葉や経済に関連する言葉が、演説コーパスで大統領によってどのように異なるかを見てみます。 クリントン以来の大統領の就任演説をコーパスとして使います：

```{r}
recentCorpus <- corpus_subset(data_corpus_inaugural, Year > 1991)
```

テロリズムと経済という2つのリストからなる辞書を作成します:
```{r}
myDict <- dictionary(list(terror = c("terrorism", "terrorists", "threat"),
                          economy = c("jobs", "business", "grow", "work")))
```


dfmを作成するときに、この辞書を使います:
```{r}
byPresMat <- dfm(recentCorpus, dictionary = myDict)
byPresMat
```

辞書のコンストラクタ関数 `dictionary()`は、2つの一般的な "外部"辞書形式でも動作します。
LIWCとWordstat (Provalis Research)形式です。 以下では、LIWCの辞書を読み込み、これを大統領就任演説コーパスに適用しています。

```{r, eval = FALSE}
liwcdict <- dictionary(file = "~/Dropbox/QUANTESS/dictionaries/LIWC/LIWC2001_English.dic",
                       format = "LIWC")
liwcdfm <- dfm(data_corpus_inaugural[52:58], dictionary = liwcdict)
liwcdfm[, 1:10]
```



# 追加の事例

## 文書の類似性

```{r fig.width = 6}
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980), 
               remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
obamaSimil <- textstat_simil(presDfm, c("2009-Obama" , "2013-Obama"), 
                             margin = "documents", method = "cosine")
obamaSimil
# dotchart(as.list(obamaSimil)$"2009-Obama", xlab = "Cosine similarity")
```


上記の、ドキュメント間の距離から樹形図を作成して、大統領をクラスター分類することができます:
```{r, fig.width = 10, fig.height = 7, eval = FALSE}
data(data_corpus_SOTU, package = "quantedaData")
presDfm <- dfm(corpus_subset(data_corpus_SOTU, Date > as.Date("1980-01-01")), 
               stem = TRUE, remove_punct = TRUE,
               remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
# hierarchical clustering - get distances on normalized dfm
presDistMat <- textstat_dist(dfm_weight(presDfm, "relfreq"))
# hiarchical clustering the distance object
presCluster <- hclust(presDistMat)
# label with document names
presCluster$labels <- docnames(presDfm)
# plot as a dendrogram
plot(presCluster, xlab = "", sub = "", main = "Euclidean Distance on Normalized Token Frequency")
```
(try it!)

用語の類似性も確かめられます:
```{r}
sim <- textstat_simil(presDfm, c("fair", "health", "terror"), method = "cosine", margin = "features")
lapply(as.list(sim), head, 10)
```

## ドキュメントのスケーリング

`textmodel()`関数にはたくさんのモデルが含まれています、ここでは "wordfish"モデルを比較した教師なしの文書スケーリングを使ってみます：

```{r}
# make prettier document names
ieDfm <- dfm(data_corpus_irishbudget2010)
textmodel_wordfish(ieDfm, dir = c(2, 1))
```

## トピックモデル

**quanteda**を使用すると、dfmを**topicmodels**の`LDA()`形式のデータに転換して、トピックモデルを簡単に推定できます：

```{r}
quantdfm <- dfm(data_corpus_irishbudget2010, 
                remove_punct = TRUE, remove_numbers = TRUE, remove = stopwords("english"))
quantdfm <- dfm_trim(quantdfm, min_count = 4, max_docfreq = 10)
quantdfm

if (require(topicmodels)) {
    myLDAfit20 <- LDA(convert(quantdfm, to = "topicmodels"), k = 20)
    get_terms(myLDAfit20, 5)
}
```
