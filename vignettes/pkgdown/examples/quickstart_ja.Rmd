---
title: "Quick Start Guide"
output: 
  html_document:
    toc: true
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = FALSE, 
                      comment = "##")
```

This vignette provides a basic overview of **quanteda**'s features and capabilities. For additional vignettes, see the [articles at quanteda.io](http://docs.quanteda.io/articles/index.html).  

# Installing the package

Since **quanteda** is available on [CRAN](https://CRAN.R-project.org/package=quanteda), you can install  by using your GUI's R package installer, or execute:

このビネットは、**quanteda**の特徴と機能の基本的な概要を提供します。 追加のビネットについては、[articles at quanteda.io](http://docs.quanteda.io/articles/index.html)を参照してください。

# パッケージのインストール

**quanteda**は[CRAN](https://CRAN.R-project.org/package=quanteda)で利用できるので、GUIのRパッケージインストーラを使用してインストールするか、次のコマンドを実行します。

```{r, eval = FALSE}
install.packages("quanteda") 
```
See an instructions at https://github.com/kbenoit/quanteda to install the GitHub version, 

GitHubのバージョンをインストールする方法については、https：//github.com/kbenoit/quantedaを参照してください。

## Additional recommended packages:

The following packages work well with or extend **quanteda** and we recommend that you also install them:

## インストールが推奨されるパッケージ

以下のパッケージは**quanteda**とうまくいったり、**quanteda**拡張されていますので、それらもインストールすることをお勧めします：

*  [**readtext**](https://github.com/kbenoit/readtext):  An easy way to read text data into R, from almost any input format.
*  [**spacyr**](https://github.com/kbenoit/spacyr): NLP using the [spaCy](http://spacy.io) library, including part-of-speech tagging, entity recognition, and dependency parsing.
*  [**quantedaData**](https://github.com/kbenoit/quantedaData): Additional textual data for use with **quanteda**.

* [**readtext**](https://github.com/kbenoit/readtext)：ほぼすべての入力形式からテキストデータをRに読み込む簡単な方法。
* [**spacyr**](https://github.com/kbenoit/spacyr)：品詞タグ付け、エンティティー認識、エンティティの認識など、[spaCy](http://spacy.io)ライブラリを使用したNLP および依存関係の解析。
* [**quantedaData**](https://github.com/kbenoit/quantedaData)：** quantedaで使用する追加のテキストデータ**。

    ```{r eval = FALSE}
    devtools::install_github("kbenoit/quantedaData")
    ```
*  [**LIWCalike**](https://github.com/kbenoit/LIWCalike): An R implementation of the [Linguistic Inquiry and Word Count](http://liwc.wpengine.com) approach to text analysis.
    ```{r eval = FALSE}
    devtools::install_github("kbenoit/LIWCalike")
    ```

# Creating a Corpus

You load the package to access to functions and data in the package.

# コーパスの作成

パッケージをロードして、パッケージ内の関数とデータにアクセスします。

```{r, message = FALSE}
library(quanteda)
```

## Currently available corpus sources

**quanteda** has a simple and powerful companion package for loading texts: [**readtext**](https://github.com/kbenoit/readtext).  The main function in this package, `readtext()`,  takes a file or fileset from disk or a URL, and returns a type of data.frame that can be used directly with the `corpus()` constructor function, to create a **quanteda** corpus object.

**quanteda**にはテキストをロードするためのシンプルで強力なコンパニオンパッケージがあります：[**readtext**](https://github.com/kbenoit/readtext) このパッケージのmain関数`readtext()`は、ディスクやURLからファイルやファイルセットを取り出し、 `corpus()`コンストラクタ関数で直接使用できるdata.frameの型を返します。 **quanteda**コーパスオブジェクト。

`readtext()` works on:

* text (`.txt`) files;
* comma-separated-value (`.csv`) files;
* XML formatted data;
* data from the Facebook API, in JSON format;
* data from the Twitter API, in JSON format; and
* generic JSON data.

The corpus constructor command `corpus()` works directly on:

* a vector of character objects, for instance that you have already loaded into the workspace using other tools;
* a `VCorpus` corpus object from the **tm** package.
* a data.frame containing a text column and any other document-level metadata.

* テキスト(`.txt`)ファイル
* コンマ区切り値(`.csv`)ファイル
* XML形式のデータ
* JSON形式のFacebook APIのデータ
* JSON形式のTwitter APIのデータ
* ジェネリックJSONデータ

コーパスコンストラクタコマンド `corpus()`は、次のものに対して直接働きます：

* 他のツールを使用してすでにワークスペースにロードしているなど、キャラクタオブジェクトのベクトル
* **tm**パッケージの `VCorpus`コーパスオブジェクト
* テキスト列と他のドキュメントレベルのメタデータを含むdata.frame

### Example: building a corpus from a character vector

The simplest case is to create a corpus from a vector of texts already in memory in R.  This gives the advanced R user complete flexbility with his or her choice of text inputs, as there are almost endless
ways to get a vector of texts into R.

If we already have the texts in this form, we can call the corpus constructor function directly.  We can demonstrate this on the built-in character object of the texts about immigration policy extracted from the 2010 election manifestos of the UK political parties (called `data_char_ukimmig2010`).

### 例：文字列からコーパスを作成

最も単純なケースは、すでにRのメモリにあるテキストのベクトルからコーパスを作成することです。これにより、高度なRユーザーは、テキスト入力を自由に選択できるようになります。
テキストのベクトルをRに入れる方法

すでにこの形式のテキストがある場合は、コーパスコンストラクタ関数を直接呼び出すことができます。 英国政党の2010年選挙宣言(「data_char_ukimmig2010」と呼ばれる)から抽出された移民政策に関するテキストの組み込みキャラクターオブジェクトでこれを実証することができます。

```{r}
myCorpus <- corpus(data_char_ukimmig2010)  # build a new corpus from the texts
summary(myCorpus)
```

If we wanted, we could add some document-level variables -- what quanteda calls `docvars` -- to this corpus.

We can do this using the R's `names()` function to get the names of the character vector `data_char_ukimmig2010`, and assign this to a document variable (`docvar`).

私たちが望むなら、いくつかのドキュメントレベルの変数(`docvars`と呼ばれるもの)をこのコーパスに追加することができます。

Rの`names()`関数を使って文字ベクトル`data_char_ukimmig2010`の名前を取得し、これをドキュメント変数(`docvar`)に代入することができます。

```{r}
docvars(myCorpus, "Party") <- names(data_char_ukimmig2010)
docvars(myCorpus, "Year") <- 2010
summary(myCorpus)
```

If we wanted to tag each document with additional meta-data not considered a document variable of interest for analysis, but rather something that we need to know as an attribute of the document, we could also 
add those to our corpus.

それぞれのドキュメントに、解析の対象となるドキュメント変数ではなく、ドキュメントの属性として知っておく必要があると思われる追加のメタデータをタグ付けしたい場合は、
それらを私たちのコーパスに追加してください。

```{r}
metadoc(myCorpus, "language") <- "english"
metadoc(myCorpus, "docsource")  <- paste("data_char_ukimmig2010", 1:ndoc(myCorpus), sep = "_")
summary(myCorpus, showmeta = TRUE)
```

The last command, `metadoc`, allows you to define your own document meta-data fields.  Note that in assiging just the single value of `"english"`, R has recycled the value until it matches the number of documents in the corpus.  In creating
a simple tag for our custom metadoc field `docsource`, we used the quanteda function `ndoc()` to retrieve
the number of documents in our corpus.  This function is deliberately designed to work in a way similar to 
functions you may already use in R, such as `nrow()` and `ncol()`.

最後のコマンド `metadoc`では、独自のドキュメントメタデータフィールドを定義することができます。 `english`の単一の値を保証する際、Rはコーパス内の文書の数と一致するまで値を再利用していることに注意してください。 作成時私たちのカスタムmetadocフィールド `docsource`のための単純なタグである、私たちはquanteda関数` ndoc() `を使ってコーパス内の文書数。 この関数は、次のような方法で動作するよう意図的に設計されています。
`nrow()`や `ncol()`のように、すでにRで使用している関数です。

### Example: loading in files using the **readtext** package

### 例：**readtext**パッケージを用いたファイルの読み込み

```{r, eval=FALSE}
require(readtext)

# Twitter json
mytf1 <- readtext("~/Dropbox/QUANTESS/social media/zombies/tweets.json")
myCorpusTwitter <- corpus(mytf1)
summary(myCorpusTwitter, 5)
# generic json - needs a textfield specifier
mytf2 <- readtext("~/Dropbox/QUANTESS/Manuscripts/collocations/Corpora/sotu/sotu.json",
                  textfield = "text")
summary(corpus(mytf2), 5)
# text file
mytf3 <- readtext("~/Dropbox/QUANTESS/corpora/project_gutenberg/pg2701.txt", cache = FALSE)
summary(corpus(mytf3), 5)
# multiple text files
mytf4 <- readtext("~/Dropbox/QUANTESS/corpora/inaugural/*.txt", cache = FALSE)
summary(corpus(mytf4), 5)
# multiple text files with docvars from filenames
mytf5 <- readtext("~/Dropbox/QUANTESS/corpora/inaugural/*.txt", 
                  docvarsfrom = "filenames", sep = "-", docvarnames = c("Year", "President"))
summary(corpus(mytf5), 5)
# XML data
mytf6 <- readtext("~/Dropbox/QUANTESS/quanteda_working_files/xmlData/plant_catalog.xml", 
                  textfield = "COMMON")
summary(corpus(mytf6), 5)
# csv file
write.csv(data.frame(inaugSpeech = texts(data_corpus_inaugural), 
                     docvars(data_corpus_inaugural)),
          file = "/tmp/inaug_texts.csv", row.names = FALSE)
mytf7 <- readtext("/tmp/inaug_texts.csv", textfield = "inaugSpeech")
summary(corpus(mytf7), 5)
```


## How a quanteda corpus works

### Corpus principles

A corpus is designed to be a "library" of original documents that have been converted to plain, UTF-8 encoded text, and stored along with meta-data at the corpus level and at the document-level.  We have a special name for document-level meta-data: *docvars*.  These are variables or features that describe attributes of each document.

A corpus is designed to be a more or less static container of texts with respect to processing and analysis.  This means that the texts in corpus are not designed to be changed internally through (for example) cleaning or pre-processing steps, such as stemming or removing punctuation.  Rather, texts can be extracted from the corpus as part of processing, and assigned to new objects, but the idea is that the corpus will remain as an original reference copy so that other analyses -- for instance those in which stems and punctuation were required, such as analyzing a reading ease index -- can be performed on the same corpus.

To extract texts from a corpus, we use an extractor, called `texts()`.  

## コーパスオブジェクトの使い方

### コーパスの原理

コーパスは、プレーンなUTF-8エンコードされたテキストに変換され、コーパスレベルとドキュメントレベルのメタデータとともに格納された元のドキュメントの「ライブラリ」として設計されています。ドキュメントレベルのメタデータには特別な名前があります：* docvars *。これらは、各文書の属性を記述する変数または機能です。

コーパスは、処理および分析に関して、テキストの多かれ少なかれ静的なコンテナになるように設計されています。これは、コーパス内のテキストが、句読点の省略や削除などの(たとえば)クリーニングや前処理ステップによって内部的に変更されるようには設計されていないことを意味します。むしろ、処理の一部としてコーパスからテキストを抽出して新しいオブジェクトに割り当てることができますが、コーパスは元の参照コピーとして残り、他の分析 - たとえば茎と句読点が必要なもの - 例えば、読書の容易さの指標を分析することは、同じコーパス上で実行することができる。

コーパスからテキストを抽出するために、我々は `texts()`と呼ばれる抽出子を使用します。

```{r}
texts(data_corpus_inaugural)[2]
```

To summarize the texts from a corpus, we can call a `summary()` method defined for a corpus.

```{r}
summary(data_corpus_irishbudget2010)
```

We can save the output from the summary command as a data frame, and plot some basic descriptive statistics with this information:

summaryコマンドの出力をデータフレームとして保存し、この情報を使っていくつかの基本的な記述統計をプロットすることができます：

```{r, fig.width = 8}
tokenInfo <- summary(data_corpus_inaugural)
if (require(ggplot2))
    ggplot(data=tokenInfo, aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() +
        scale_x_discrete(labels = c(seq(1789,2012,12)), breaks = seq(1789,2012,12) ) 

# Longest inaugural address: William Henry Harrison
tokenInfo[which.max(tokenInfo$Tokens), ] 
```


## Tools for handling corpus objects

### Adding two corpus objects together

The `+` operator provides a simple method for concatenating two corpus objects.  If they contain
different sets of document-level variables, these will be stitched together in a fashion that guarantees
that no information is lost.  Corpus-level medata data is also concatenated.

## コーパスに対する操作

### コーパスの結合

`+`演算子は、2つのコーパスオブジェクトを連結する簡単な方法を提供します。含まれている場合ドキュメントレベルの変数の異なるセットは、これらは、保証された方法で一緒に縫い合わされます情報が失われないこと。 コーパスレベルのメデアデータも連結されています。

```{r}
library(quanteda)
mycorpus1 <- corpus(data_corpus_inaugural[1:5])
mycorpus2 <- corpus(data_corpus_inaugural[53:58])
mycorpus3 <- mycorpus1 + mycorpus2
summary(mycorpus3)
```

### Subsetting corpus objects

There is a method of the `corpus_subset()` function defined for corpus objects, where a new corpus can 
be extracted based on logical conditions applied to docvars:

### コーパス内の文書の選択

コーパスオブジェクトのために定義された `corpus_subset()`関数のメソッドがあります。ここでは新しいコーパスが
docvarsに適用される論理条件に基づいて抽出されます。

```{r}
summary(corpus_subset(data_corpus_inaugural, Year > 1990))
summary(corpus_subset(data_corpus_inaugural, President == "Adams"))
```


## Exploring corpus texts

The `kwic` function (keywords-in-context) performs a search for a word and allows us to view the contexts in which it occurs:

## コーパス内の文書の探検

`kwic`関数(文脈の中のキーワード)は単語の検索を行い、それが起こっている文脈を見ることができます：

```{r, tidy=TRUE}
kwic(data_corpus_inaugural, "terror")
```

```{r}
kwic(data_corpus_inaugural, "terror", valuetype = "regex")
```

```{r}
kwic(data_corpus_inaugural, "communist*")
```


In the above summary, `Year` and `President` are variables associated with each document. We can access such variables with the `docvars()` function.

上記の要約では、「年」と「社長」は各文書に関連する変数です。 `docvars()`関数でこのような変数にアクセスできます。

```{r}
# inspect the document-level variables
head(docvars(data_corpus_inaugural))

# inspect the corpus-level metadata
metacorpus(data_corpus_inaugural)
```

More corpora are available from the [quantedaData](http://github.com/kbenoit/quantedaData) package.




# Extracting Features from a Corpus

In order to perform statistical analysis such as document scaling, we
must extract a matrix associating values for certain features with each
document. In quanteda, we use the `dfm` function to produce such a matrix.  "dfm" is short for *document-feature matrix*, and always refers to documents
in rows and "features" as columns.  We fix this dimensional orientation because it is 
standard in data analysis to have a unit of analysis as a row, and features or variables
pertaining to each unit as columns.  We call them "features" rather than terms, because
features are more general than terms: they can be defined as raw terms, stemmed terms, the parts of speech of terms, terms after stopwords have been removed,
or a dictionary class to which a term belongs.  Features can be entirely general, such as ngrams or syntactic dependencies, and we leave this open-ended.

# コーパスから特徴を抽出

文書スケーリングなどの統計分析を行うために、特定のフィーチャの値をそれぞれに関連付ける行列を抽出する必要があります資料。 quantedaでは、このような行列を生成するために `dfm`関数を使います。 "dfm"は*document-feature matrix *の略で、常にドキュメントを参照します
列には「フィーチャ」が表示されます。 この次元の向きは固定されています
データ分析の標準であり、分析単位を行として持ち、特徴または変数
列として各ユニットに関連する。 我々はそれらを用語ではなく「機能」と呼んでいる。
機能は用語よりも一般的です：それらは、生の用語、ステムされた用語、用語の品詞、ストップワードが削除された後の用語、
または用語が属する辞書クラスである。 機能は、ngramsや構文依存性などの一般的なものにすることができ、私たちはこれをオープンエンドにします。

## Tokenizing texts

To simply tokenize a text, quanteda provides a powerful command called `tokens()`.  This produces an 
intermediate object, consisting of a list of tokens in the form of character vectors, where each element
of the list corresponds to an input document.

`tokens()` is deliberately conservative, meaning that it does not remove anything from the text unless
told to do so.

## 文書のトークン化

単純にテキストをトークン化するために、quantedaは `tokens()`と呼ばれる強力なコマンドを提供します。 これにより、中間オブジェクト。文字ベクトルの形式のトークンのリストで構成され、各要素リストのうちの1つは入力文書に対応する。

`tokens()`は故意に保守的です。つまり、そうするように言われた。

```{r}
txt <- c(text1 = "This is $10 in 999 different ways,\n up and down; left and right!", 
         text2 = "@kenbenoit working: on #quanteda 2day\t4ever, http://textasdata.com?page=123.")
tokens(txt)
tokens(txt, remove_numbers = TRUE,  remove_punct = TRUE)
tokens(txt, remove_numbers = FALSE, remove_punct = TRUE)
tokens(txt, remove_numbers = TRUE,  remove_punct = FALSE)
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE)
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE, remove_separators = FALSE)
```

We also have the option to tokenize characters:
```{r}
tokens("Great website: http://textasdata.com?page=123.", what = "character")
tokens("Great website: http://textasdata.com?page=123.", what = "character", 
         remove_separators = FALSE)
```

and sentences:
```{r}
# sentence level         
tokens(c("Kurt Vongeut said; only assholes use semi-colons.", 
           "Today is Thursday in Canberra:  It is yesterday in London.", 
           "En el caso de que no puedas ir con ellos, ¿quieres ir con nosotros?"), 
          what = "sentence")
```

## Constructing a document-feature matrix

Tokenizing texts is an intermediate option, and most users will want to skip straight to constructing
a document-feature matrix.  For this, we have a Swiss-army knife function, called `dfm()`, which performs
tokenization and tabulates the extracted features into a matrix of documents by features.  Unlike
the conservative approach taken by `tokens()`, the `dfm()` function applies certain options by default,
such as `toLower()` -- a separate function for lower-casing texts -- and removes punctuation.  All of the options to `tokens()` can be passed to `dfm()`, however.

## 文書行列の作成

テキストをトークン化することは中間的な選択肢であり、ほとんどのユーザーはスキップして直感的に
ドキュメントフィーチャマトリックス このために、私たちは `dfm()`と呼ばれるスイス軍ナイフ機能を持っています。
抽出された特徴をフィーチャによって文書のマトリックスに集計する。 とは異なり
`tokens()`、 `dfm()`関数が保守的に取り組んでいるアプローチは、デフォルトでいくつかのオプションを適用しています。
`toLower()` - 小文字のテキストのための別の関数 - や句読点を削除するなど)。 ただし、 `tokens()`のオプションはすべて `dfm()`に渡すことができます。

```{r}
myCorpus <- corpus_subset(data_corpus_inaugural, Year > 1990)

# make a dfm
myDfm <- dfm(myCorpus)
myDfm[, 1:5]
```

Other options for a `dfm()` include removing stopwords, and stemming the tokens.

`dfm()`のその他のオプションには、ストップワードの削除やトークンのステミングが含まれます。

```{r}
# make a dfm, removing stopwords and applying stemming
myStemMat <- dfm(myCorpus, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
myStemMat[, 1:5]
```

The option `remove` provides a list of tokens to be ignored.  Most users will 
supply a list of pre-defined "stop words", defined for numerous languages, accessed through 
the `stopwords()` function:

`remove`オプションは、無視されるトークンのリストを提供します。 ほとんどのユーザーは
多数の言語に定義された定義済みの「ストップワード」のリストを、
`stopwords()`関数：

```{r}
head(stopwords("english"), 20)
head(stopwords("russian"), 10)
head(stopwords("arabic"), 10)
```



### Viewing the document-feature matrix

The dfm can be inspected in the Enviroment pane in RStudio, or by calling R's `View` function. Calling `plot` on a dfm will display a wordcloud using the [wordcloud package](https://cran.r-project.org/web/packages/wordcloud)

### 文書行列の表示

dfmは、RStudioの環境ペインで、またはRの `View`関数を呼び出すことで調べることができます。 dfmで `plot`を呼び出すと、[wordcloudパッケージ](https://cran.r-project.org/web/packages/wordcloud)を使ってワードクラウドが表示されます。

```{r warning=FALSE, fig.width = 8, fig.height = 8}
mydfm <- dfm(data_char_ukimmig2010, remove = stopwords("english"), remove_punct = TRUE)
mydfm
```

To access a list of the most frequently occurring features, we can use `topfeatures()`:
```{r}
topfeatures(mydfm, 20)  # 20 top words
```

Plotting a word cloud is done using `textplot_wordcloud()`, for a `dfm` class object.  This function passes arguments through to `wordcloud()` from the **wordcloud** package, and can prettify the plot using the same arguments:
```{r warning=FALSE, fig.width = 7, fig.height = 7}
set.seed(100)
textplot_wordcloud(mydfm, min.freq = 6, random.order = FALSE,
                   rot.per = .25, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"))
```

### Grouping documents by document variable 

Often, we are interested in analysing how texts differ according to substantive factors which may be encoded in the document variables, rather than simply by the boundaries of the document files. We can group documents which share the same value for a document variable when creating a dfm:

### 文書変数による文書のグループ化

多くの場合、文書ファイルの境界線ではなく、文書変数でエンコードされる可能性のある実質的要因によってテキストがどのように異なるかを分析することに興味があります。 dfmを作成するときに、ドキュメント変数と同じ値を共有するドキュメントをグループ化できます。

```{r}
byPartyDfm <- dfm(data_corpus_irishbudget2010, groups = "party", 
                  remove = stopwords("english"), remove_punct = TRUE)
```

We can sort this dfm, and inspect it:
```{r}
dfm_sort(byPartyDfm)[, 1:10]
```

Note that the most frequently occurring feature is "will", a word usually on English stop lists, but
one that is not included in quanteda's built-in English stopword list.  

最も頻繁に出現する特徴は、「英語」のストップリストにある単語である「意志」ですが、
quantedaの組み込み英語ストップワードリストには含まれていません。

### Grouping words by dictionary or equivalence class

For some applications we have prior knowledge of sets of words that are indicative of traits we would like to measure from the text. For example, a general list of positive words might indicate positive sentiment in a movie review, or we might have a dictionary of political terms which are associated with a particular ideological stance. In these cases, it is sometimes useful to treat these groups of words as equivalent for the purposes of analysis, and sum their counts into classes. 

For example, let's look at how words associated with terrorism and words associated with the economy vary by President in the inaugural speeches corpus. From the original corpus, we select Presidents since Clinton:

### 辞書を用いた語のグループ化

いくつかのアプリケーションでは、テキストから測定したい形質の指標である単語のセットについての事前知識があります。 たとえば、肯定的な言葉の一般的なリストは、映画レビューの肯定的な感情を示すかもしれない、または我々は特定のイデオロギー的なスタンスに関連付けられている政治用語の辞書を持っている可能性があります。 このような場合、分析目的でこれらの単語のグループを同等のものとして扱い、カウントをクラスに合計すると便利な場合があります。

たとえば、テロリズムに関連する言葉や経済に関連する言葉が、演説コーパスで大統領によってどのように異なるかを見てみましょう。 元のコーパスからは、クリントン以来の大統領選を選びます：

```{r}
recentCorpus <- corpus_subset(data_corpus_inaugural, Year > 1991)
```

Now we define a demonstration dictionary:
```{r}
myDict <- dictionary(list(terror = c("terrorism", "terrorists", "threat"),
                          economy = c("jobs", "business", "grow", "work")))
```


We can use the dictionary when making the dfm:
```{r}
byPresMat <- dfm(recentCorpus, dictionary = myDict)
byPresMat
```

The constructor function `dictionary()` also works with two common "foreign" dictionary formats: the 
LIWC and Provalis Research's Wordstat format.  For instance, we can load the LIWC and apply this to the Presidential inaugural speech corpus:

コンストラクタ関数 `dictionary()`は、2つの一般的な "外部"辞書形式でも動作します。
LIWCとProvalis ResearchのWordstat形式。 たとえば、LIWCを読み込み、これを大統領就任演説コーパスに適用することができます。

```{r, eval = FALSE}
liwcdict <- dictionary(file = "~/Dropbox/QUANTESS/dictionaries/LIWC/LIWC2001_English.dic",
                       format = "LIWC")
liwcdfm <- dfm(data_corpus_inaugural[52:58], dictionary = liwcdict)
liwcdfm[, 1:10]
```



# Further Examples

## Similarities between texts

# 追加の事例

## 文書の類似性

```{r fig.width = 6}
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980), 
               remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
obamaSimil <- textstat_simil(presDfm, c("2009-Obama" , "2013-Obama"), 
                             margin = "documents", method = "cosine")
obamaSimil
# dotchart(as.list(obamaSimil)$"2009-Obama", xlab = "Cosine similarity")
```


We can use these distances to plot a dendrogram, clustering presidents:
```{r, fig.width = 10, fig.height = 7, eval = FALSE}
data(data_corpus_SOTU, package = "quantedaData")
presDfm <- dfm(corpus_subset(data_corpus_SOTU, Date > as.Date("1980-01-01")), 
               stem = TRUE, remove_punct = TRUE,
               remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
# hierarchical clustering - get distances on normalized dfm
presDistMat <- textstat_dist(dfm_weight(presDfm, "relfreq"))
# hiarchical clustering the distance object
presCluster <- hclust(presDistMat)
# label with document names
presCluster$labels <- docnames(presDfm)
# plot as a dendrogram
plot(presCluster, xlab = "", sub = "", main = "Euclidean Distance on Normalized Token Frequency")
```
(try it!)

We can also look at term similarities:
```{r}
sim <- textstat_simil(presDfm, c("fair", "health", "terror"), method = "cosine", margin = "features")
lapply(as.list(sim), head, 10)
```

## Scaling document positions

We have a lot of development work to do on the `textmodel()` function, but here is a demonstration of unsupervised document scaling comparing the "wordfish" model: 

## 文書の位置の特定

`textmodel()`関数にはたくさんの開発作業がありますが、ここでは "wordfish"モデルを比較した教師なしの文書スケーリングのデモンストレーションがあります：

```{r}
# make prettier document names
ieDfm <- dfm(data_corpus_irishbudget2010)
textmodel_wordfish(ieDfm, dir = c(2, 1))
```

## Topic models

**quanteda** makes it very easy to fit topic models as well, e.g.:

## トピックモデル

**quanteda**を使用すると、トピックモデルにも簡単にフィットできます(例：

```{r}
quantdfm <- dfm(data_corpus_irishbudget2010, 
                remove_punct = TRUE, remove_numbers = TRUE, remove = stopwords("english"))
quantdfm <- dfm_trim(quantdfm, min_count = 4, max_docfreq = 10)
quantdfm

if (require(topicmodels)) {
    myLDAfit20 <- LDA(convert(quantdfm, to = "topicmodels"), k = 20)
    get_terms(myLDAfit20, 5)
}
```
