---
title: "tokens() functionality comparison"
date: "22/03/2016"
output: html_document
---
To demonstrate the functionality of `tokens()` that removes or keeps some special characters/symbols. Test on: 

```{r}
require(quanteda, quietly = TRUE, warn.conflicts = FALSE)
require(tokenizers, quietly = TRUE, warn.conflicts = FALSE)
poetry <- paste0("I wandered lonely as a cloud,\n",
                 "That floats on high o'er vales and hills.\n",
                 "They stretched in never-ending line.\n",
                 "Tossing their heads in sprightly @LSE and tell us what makes you feel #partofLSE.\n",
                 "\n",
                 "1 $100 £1000 2000+. \n",
                 "Prof. Plum kills Mrs. Peacock. \n",
                 "4u @ http://www.github.com\n")
```
### Preserve words with hyphens
```{r}
tokens("They stretched in never-ending line.\n", what = "word", removePunct = TRUE, removeHyphens = FALSE)
tokenize_words("They stretched in never-ending line.\n")
```

### Eliminate URLs beginning with "http(s)" 
```{r}
tokens("4u http://www.github.com\n", what = "word", removePunct = TRUE, removeURL = TRUE)
tokenize_words("4u http://www.github.com\n")
```

### Preserve Twitter characters @ and \#
```{r}
tokens("in sprightly @LSE and tell us what makes you feel #partofLSE\n", what = "word", removePunct = TRUE, removeTwitter = FALSE)
tokenize_words("in sprightly @LSE and tell us what makes you feel #partofLSE\n")
```

### Remove numbers but preserve words starting with digits
```{r}
tokens(c("1 $100 £1000 2000+ \n", "4u http://www.github.com\n"), what = "word", removePunct = TRUE, removeNumbers = TRUE)
tokenize_words(c("1 $100 £1000 2000+ \n", "4u http://www.github.com\n"))
```

### Remove Symbols in the Unicode "Symbol" [S] class
```{r}
tokens("1 $ 100 £1000 2000+", what = "character", removePunct = TRUE, removeSymbols = TRUE)
tokenize_characters("1 $ 100 £1000 2000+", strip_non_alphanum = TRUE)
```

### Remove Separators in the Unicode "Separator" [Z] class but not Punctuations
```{r}
tokens("1 $ 100 £1000 2000+\n", what = "word", removePunct = FALSE, removeSeparators = TRUE)
tokenize_words("1 $ 100 £1000 2000+\n")
```

### Sentence segmenter handles some exceptions in English
```{r}
tokens(poetry, what = "sentence")
tokenize_sentences(poetry)
```
### Performance benchmark
```{r}
microbenchmark::microbenchmark(q_tokens = tokens(poetry, what = "word", hash = FALSE, removeSeparators = FALSE),
                               tokenizers = tokenize_words(poetry), 
                               times = 20, unit = "relative")

microbenchmark::microbenchmark(q_tokens = tokens(poetry, what = "fasterword", hash = FALSE, removeSeparators = FALSE),
                               tokenizers = tokenize_words(poetry), 
                               times = 20, unit = "relative")

microbenchmark::microbenchmark(q_tokens = tokens(poetry, what = "fastestword", hash = FALSE, removeSeparators = FALSE),
                               tokenizers = tokenize_words(poetry), 
                               times = 20, unit = "relative")
```
